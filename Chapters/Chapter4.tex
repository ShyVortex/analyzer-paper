% Chapter 4

\chapter{Empirical Study} % Main chapter title

\label{Chapter4}

%----------------------------------------------------------------------------------------

In this chapter, we present an empirical study aimed at validating the effectiveness of the tool that was developed. We want to create a highly precise system capable of detecting previously outlined deficiencies in code comments, ensuring accurate identification of such.

\section{Study Design}
The goal of this study is to understand to what extent the proposed heuristics allow to detect true comments lacking quality. Our study is steered by the following research
question:
\begin{large}
	\begin{Center}
		\textbf{RQ:} How accurate are our heuristics in detecting poor-quality comment smells in state-of-the-art datasets?
	\end{Center}
\end{large}

\subsection{Data Collection}
In order to validate our heuristic, we focused on \textit{CodeSearchNet} (CSN) \cite{CSN}, which is vastly used in literature, selecting a random sample of 384 instances.
This number is important as a sample of that size guarantees Â±5\% margin of error and 95\% confidence level.

\noindent CSN is a large-scale dataset designed to facilitate semantic code search, which is the process of retrieving relevant code snippets based on natural language queries.
The dataset includes over 6 million functions from six programming languages, but our subset focuses solely on \textit{Java}.

\subsection{Data Analysis}
We manually analyzed comments from the sample, labeling each with a value of 0 when the comment was not meeting a detection criteria, and 1 otherwise. In ambiguous cases, we defaulted to 0.
For the purpose of validating our heuristic, we compared its results against our manual annotations, using the metrics of accuracy, precision, recall, and f1score for each criteria. These metrics were computed with the following formulas:

\begin{equation*}
	accuracy = \frac{TP + TN}{TP + TN + FP + FN}		
\end{equation*}
\begin{equation*}
	precision = \frac{TP}{TP + FP}
\end{equation*}
\begin{equation*}
	recall = \frac{TP}{TP + FN}
\end{equation*}
\begin{equation*}
	f1score = 2 \times \frac{precision \times recall}{precision + recall}
\end{equation*}

\noindent With the term TP we indicate those comments that are caught by our heuristics in one of the detection criteria. With the term TN we indicate those comments that are not caught by our heuristics in one or any of the detection criteria. With the term FP we indicate those comments that are detected by our heuristic in one of the criteria but actually should not. With the term FN we indicate those comments that are not detected by our heuristic in one or any of the criteria but actually should.

\section{Study Results}
After carefully reviewing the sample multiple times, we arrived at the following results:

\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		& \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
		\hline Comments Classification*    & 84\%     & 87\%      & 85\%   & 86\%    \\
		\hline Empty Comments             & 97\%     & 63\%      & 67\%   & 65\%    \\
		\hline Comments Asking Questions  & 98\%     & 40\%      & 33\%   & 36\%    \\
		\hline Short Comments             & 97\%     & 88\%      & 98\%   & 93\%    \\
		\hline Long Comments              & 90\%     & 89\%      & 92\%   & 91\%    \\
		\hline Comments Under Development & 96\%     & 83\%      & 58\%   & 68\%    \\
		\hline Incomplete Comments        & 86\%     & 94\%      & 89\%   & 92\%    \\
		\hline Uneven Comments Format     & 87\%     & 93\%      & 83\%   & 88\%   \\
		\hline
	\end{tabular}
	
	\textit{\\}
	
	*\textit{Comments Classification} refers to the detector that is able to scan all comments and categorize them in single-line, multi-line and documentation. See paragraph $\ref{sec:logic-construction}$.
\end{center}

\noindent From the table above, it can be deduced that our heuristics generally perform well, showing strong metrics in most cases. However, there are some notable caveats. The results for \textit{Empty Comments}, \textit{Comments Asking Questions}, and \textit{Comments Under Development} are visibly less balanced, with lower precision, recall, and F1 scores. This is primarily due to the small number of true positives in these categories, which has skewed the metrics.

\noindent To better understand this issue, we can refer to the confusion matrix values for these categories:
\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		& \textbf{TP} & \textbf{TN} & \textbf{FP} & \textbf{FN} \\
		\hline Empty Comments             & 10     & 362      & 6   & 5    \\
		\hline Comments Asking Questions  & 2     & 375      & 3   & 4    \\
		\hline Comments Under Development & 15     & 355      & 3   & 11    \\
		\hline
	\end{tabular}
\end{center}

\noindent \\ As we can see, the number of true positives (TP) is relatively low in these cases, which significantly impacts the precision, recall, and F1 score. For example, only 2 true positives were identified for \textit{Comments Asking Questions}, which contributes to the low scores despite the high accuracy. Meanwhile, \textit{Comments Under Development} shows a relatively high number of false negatives (FP) which affects recall and F1 score despite high accuracy and precision.

\section{Threats to Validity}
This section discusses the threats to validity of this heuristic. Specifically, (I) construction validity regarding manual testing, (II) internal validity regarding possible impairment of results and (III) external validity regarding the dataset.

\subsection{Construct Validity}
We have detailed the logic behind our heuristics and described the validation process used to evaluate their effectiveness by extracting key metrics. The heuristics were tested on 384 instances from the \textit{CSN} dataset. This process involved manually labeling each comment, meaning that any misinterpretation of the comments could potentially compromise the validity of the results. To mitigate this risk, we thoroughly reviewed and examined all comments multiple times to ensure accuracy and reduce the likelihood of errors in interpretation.

\subsection{Internal Validity}
To address the research question, we developed heuristics grounded in our knowledge of the domain. This approach involved incorporating our own insights and patterns related to comment analysis, which may have introduced some subjectivity, particularly in the more complex detectors, such as those for incomplete comments and uneven comment formatting. However, we aimed to select rules that are broadly applicable and objective, while maintaining a disciplined approach to minimize subjectivity and ensure consistency in our criteria.

\noindent We acknowledge that with different samples, results might vary, potentially yielding more balanced or imbalanced outcomes. However, the current analysis provides valuable insights into where improvements could be made in future iterations.

\subsection{External Validity}
We selected small, random samples from a very large dataset, which resulted in analyzing only a limited number of instances relative to the dataset size. This sampling approach may have led to some imbalances in the results. However, our primary goal was to design heuristics that minimized false positives and false negatives. Although we tested these heuristics across a variety of comment categories, certain categories had very few relevant instances, suggesting that the results could be influenced by the specific sample chosen and may not fully generalize to the entire dataset.
Additionally, factors such as the programming language used and the presence of external frameworks, which often fall outside the built-in syntax and conventions of the language, could further affect the generalizability of our findings. These factors may limit the ability to extend our results to other datasets or environments without further validation.