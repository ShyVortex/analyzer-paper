% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Application Context} 
Large language models (LLMs) have emerged as powerful tools in the field of software development, capable of performing a wide range of coding tasks with impressive accuracy. From generating code snippets and auto-completing functions to identifying bugs and suggesting optimizations, LLMs have become fundamental in streamlining the software development process. They are now being employed in integrated development environments (IDEs), facilitating rapid prototyping, code refactoring, and even explaining code behavior in natural language.
However, the quality of code generated by LLMs is highly dependent on the datasets used to train these models. LLMs learn to write code by analyzing vast repositories of source code, comments, and documentation available across different programming languages. If the training data contains high-quality, well-structured code with meaningful comments, the models are more likely to generate precise, readable, and efficient code \cite{dlCR}. On the other hand, poor-quality training data containing incomplete, incorrect, or poorly commented code can lead to less reliable outputs.
While comments do not affect the program’s execution, they play an essential role in this context \cite{codeReadability}  \cite{codeMaintainability}, critical to software development as a whole in both small teams and large organizations.

\noindent Comments are non-executable statements embedded within code to provide explanations, clarify intent, and guide future developers or collaborators who may interact with the codebase.
When writing comments, at first glance, general principles and good practices \cite{commentingPrinciples} are often overlooked. First of all, it is important to strike a balance in their length, ensuring they are neither too brief nor overly detailed. Comments should be concise yet informative, providing enough context without overwhelming the reader with unnecessary details.
For documentation comments, developers must adhere to the specific syntax and formatting rules of the programming language being used, particularly when it comes to annotations and tags, to ensure proper integration with documentation tools.
Technical comments added during development should be revised or removed when they become obsolete. Outdated or irrelevant comments can lead to confusion and misinterpretation.
Additionally, comments should avoid posing questions, as their primary function is to clarify and guide, not to introduce uncertainty. Lastly, comments must not be left empty, as they serve no purpose and only add clutter to the codebase.

\noindent Given these premises, it is therefore imperative to make sure comments are verified to be correctly written, with as high quality as possible.


\section{Motivation and Objectives}
Comments play an important role in improving the performance of code completion models \cite{vandam2023}.
The presence of multi-line comments was found to significantly enhance the performance of pre-trained language models like \textit{UniXcoder}, \textit{CodeGPT}, and \textit{InCoder}. \textit{Van Dam et al.} suggest that natural language descriptions embedded in multi-line comments contribute to the models' ability to understand and complete code, making these comments valuable in datasets used for training and evaluating code-related tasks.

\noindent Comments are also important in the context of code summarization tasks \cite{buildingRock}.
High-quality comments enhance the ability of code summarization models to generate accurate and useful descriptions of source code. However, it is also noted how noisy or poor-quality comments can significantly reduce the effectiveness of these models.

\noindent Given their importance in model training for code-tasks, it was necessary and important to perform an in-depth study, taking on this challenge. Developing a comment analysis system \cite{commentAnalysis} is not an easy task, as it requires a deep understanding of the rules governing the programming language in which the code is written, alongside ensuring computational accuracy and efficiency when processing comment data.

\noindent Additionally, the content of comments can often be complex or ambiguous, making it difficult for automated heuristics to handle certain scenarios correctly. In such cases, manual review may be necessary to address edge cases or interpret unclear comments.
Therefore, it is crucial to ensure that the system is robust and effective for the vast majority of use cases, while acknowledging that some instances may require additional intervention.

\section{Results}
The first result is a tool to analyze the quality of comments in a given dataset. We have applied NLP techniques in some cases to enhance the quality of the output, conducting an empirical study to evaluate the effectiveness of our approach on sample datasets consisting of manually selected instances.

\noindent Then we compared the automated results produced by our tool with a thorough manual analysis of the same sample, trying to ensure the error rate of our tool stayed within a ±5\% margin. 
After analyzing the aforementioned datasets, we discovered, for instance, that a significant portion of comments are excessively long, and approximately 20\% of documentation comments are left incomplete. Finally, we identified the key limitations of our approach and provided recommendations for future research and improvements. The main contributions of this thesis can be summarized as follows:
(I) we proposed an analysis tool for automatic detection of lack of comments quality, evaluated under different criteria,
(II) we used a set of carefully selected datasets to train our approach,
(III) we defined case-study samples from selected datasets to verify the correctness of our approach.

\section{Thesis Structure and Organization}
The next chapters of this thesis are:

\begin{itemize}
\item \capref{Chapter2}: presents background and related works.
\item \capref{Chapter3}: presents our tool for detecting lack of comments quality in each category and the heuristics used.
\item \capref{Chapter4}: presents the empirical study conducted to validate the
defined heuristic.
\item \capref{Chapter5}: concludes this thesis and provides directions for future works.
\end{itemize}